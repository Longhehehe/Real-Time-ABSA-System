{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bài thực hành lấy điểm lần 1: Phân lớp văn bản tiếng Anh:**\n",
        "\n",
        "*   Họ tên: Đỗ Tấn Lực\n",
        "*   Mã số sinh viên: 23520903\n",
        "*   Lớp: CS221.P21\n",
        "\n"
      ],
      "metadata": {
        "id": "NnyOx9_dV-0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Cài đặt các thư viện."
      ],
      "metadata": {
        "id": "zd4BNAQTWaWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import math\n",
        "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFoyxd5QWRSh",
        "outputId": "2737d1c0-02c9-4dca-c570-e747d5895e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp theo, ta load dữ liệu văn bản tiếng Anh vào bằng thư viện Pandas."
      ],
      "metadata": {
        "id": "rwIohKO7XrLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('example.csv')\n",
        "data.info()"
      ],
      "metadata": {
        "id": "XdW5Iau6XGRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta thống kê sơ bộ về ngữ liệu, về số lượng mẫu, datapoint mỗi class."
      ],
      "metadata": {
        "id": "j0Xu94FPY471"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols= sns.color_palette(\"Reds\", n_colors=2)\n",
        "plt.figure(figsize=(10,6))\n",
        "fig = sns.countplot(data=data, x='Class', hue=\"Class\", palette= cols, legend=False)\n",
        "fig.set_title(\"Thống kê số mẫu theo lớp\", color=\"#000000\")\n",
        "fig.set_xlabel(\"Lớp\")\n",
        "fig.set_ylabel(\"Số mẫu\")"
      ],
      "metadata": {
        "id": "1U23gr3vZB3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tiền xử lí dữ liệu."
      ],
      "metadata": {
        "id": "nW0Jfa_GZOhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Tokenize"
      ],
      "metadata": {
        "id": "dpQ6E7nlBWKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp theo, là tiền xử lý dữ liệu, ở đây là các văn bản tiếng Anh. Chúng ta sẽ thực hiện:\n",
        "\n",
        "- Chuyển văn bản về chữ thường.\n",
        "- Loại bỏ các kí tự không phải chữ cái.\n",
        "- Tách từ.\n",
        "- Dùng Stemming/Lemmatization chuyển văn bản về dạng gốc."
      ],
      "metadata": {
        "id": "GvSZGfGrbyBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, use_lemmatizer=False):\n",
        "    text = text.lower() # Chuyển thành chữ thường\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) # Loại bỏ các ký tự đặc biệt\n",
        "    tokens = text.split() # Tokenize theo khoảng trắng\n",
        "    stop_words = set(stopwords.words('english')) # Loại bỏ stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming/Lemmatization\n",
        "    if use_lemmatizer:\n",
        "        # WordNetLemmatizer\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    else:\n",
        "        # PorterStemmer\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "gGMOx1rCZUQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Vectorize"
      ],
      "metadata": {
        "id": "ApNB-Y3KBZbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp theo, ta xây dựng class TF-IDF để vectorize văn bản theo tần suất."
      ],
      "metadata": {
        "id": "9VHHCoqLc4Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TF_IDF:\n",
        "  def __init__(self):\n",
        "    self.vocab = {} # Từ điển lưu cặp giá trị: token - index\n",
        "    self.vocab_size = 0 # Kích thước từ điển\n",
        "    self.idf = {} # Lưu giá trị IDF cho mỗi token\n",
        "\n",
        "  def fit(self, documents):\n",
        "    total_token = set() # Lưu trữ từ điển của documents\n",
        "    doc_freq = defaultdict(int) # Lưu trữ số document chứa token\n",
        "    for doc in documents:\n",
        "      unique_token = set(doc)\n",
        "      for token in unique_token:\n",
        "        doc_freq[token] +=1\n",
        "        total_token.add(token)\n",
        "\n",
        "    self.vocab = {token: id for id, token in enumerate(sorted(total_token))}\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    total_doc = len(documents)\n",
        "\n",
        "    for token, frequence in doc_freq.items():\n",
        "      self.idf[token] = math.log(total_doc +1 / frequence + 1) + 1 # Tính giá trị IDF của từng token\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, documents):\n",
        "    X = np.zeros((len(documents), self.vocab_size))\n",
        "    for doc_id, doc in enumerate(documents):\n",
        "      count_token = Counter(doc) # Đếm tần suất của token\n",
        "      for token, frequence in count_token.items():\n",
        "        if token in self.vocab:\n",
        "          X[doc_id, self.vocab[token]] = frequence * self.idf[token] # Tính TFIDF cho từ điển\n",
        "\n",
        "    return X\n",
        "\n",
        "  def fit_transform(self, documents):\n",
        "    self.fit(documents)\n",
        "    return self.transform(documents)"
      ],
      "metadata": {
        "id": "qd5KUO60csDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cài đặt các mô hình."
      ],
      "metadata": {
        "id": "99qTZuAxS0SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "ZZx3i7OCTBTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def __init__(self, alpha=1):\n",
        "        self.alpha = alpha # Smoothing\n",
        "        self.class_prob = None # Xác suất tiên nghiệm P(y)\n",
        "        self.feature_prob = None # Xác suất đặc trưng xuất hiện trong mỗi class P(x | y)\n",
        "        self.classes = None\n",
        "        self.n_features = None\n",
        "        self.feature_count = None\n",
        "        self.class_count = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if not isinstance(y, np.ndarray):\n",
        "            y = np.array(y)\n",
        "\n",
        "        # Get dimensions and unique classes\n",
        "        n_samples, self.n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        self.class_count = np.zeros(n_classes)\n",
        "        for i, c in enumerate(self.classes):\n",
        "            self.class_count[i] = np.sum(y == c)\n",
        "        self.class_prob = self.class_count / n_samples # Tính xác suất tiên nghiệm P(y)\n",
        "\n",
        "        self.feature_count = np.zeros((n_classes, self.n_features)) # Lưu trữ số đặc trưng\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_c = X[y == c] # Lấy tất cả documents thuộc từng class\n",
        "            self.feature_count[i] = X_c.sum(axis=0) # Tính tổng số đặc trưng theo từng class\n",
        "        self.total_words_per_class = np.sum(self.feature_count, axis=1) # Tính tổng số từ trong mỗi class\n",
        "\n",
        "        self.feature_prob = np.zeros((n_classes, self.n_features))\n",
        "        for i in range(n_classes):\n",
        "            self.feature_prob[i] = (self.feature_count[i] + self.alpha) / (self.total_words_per_class[i] + self.alpha * self.n_features) # Tính xác suất đặc trưng xuất hiện trong class - Laplace Smoothing\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes[np.argmax(self.log_likelihood(X), axis=1)] # Lấy class có xác suất cao nhất\n",
        "\n",
        "    def log_likelihood(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        log_likelihood = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for i, c in enumerate(self.classes):\n",
        "            log_likelihood[:, i] = np.log(self.class_prob[i]) # Tính log của xác suất tiên nghiệm log(P(y))\n",
        "            if isinstance(X, np.ndarray):\n",
        "                feature_prob_c = self.feature_prob[i]\n",
        "                log_probs = np.log(np.maximum(feature_prob_c, 1e-10)) # Thêm max để tránh log0\n",
        "                log_likelihood[:, i] += np.dot(X, log_probs)\n",
        "            else:\n",
        "                for j in range(n_samples):\n",
        "                    for feature_idx, count in zip(X[j].indices, X[j].data):\n",
        "                        prob = self.feature_prob[i, feature_idx]\n",
        "                        if prob > 0:\n",
        "                            log_likelihood[j, i] += count * math.log(prob)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return accuracy_score(y, y_pred)"
      ],
      "metadata": {
        "id": "6DS_m7xaQVf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Bernouli Naive Bayes\n"
      ],
      "metadata": {
        "id": "PZkF2lQAT2fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BernoulliNaiveBayes:\n",
        "    def __init__(self, alpha=1):\n",
        "        self.alpha = alpha\n",
        "        self.class_prob = None\n",
        "        self.feature_prob = None\n",
        "        self.n_feature = None\n",
        "        self.classes = None\n",
        "        self.class_count = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if not isinstance(y, np.ndarray):\n",
        "            y = np.array(y)\n",
        "\n",
        "        n_sample, self.n_feature = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_class = len(self.classes)\n",
        "\n",
        "        self.class_count = np.zeros(n_class)\n",
        "        self.class_prob = np.zeros(n_class)\n",
        "\n",
        "        for i, c in enumerate(self.classes):\n",
        "            self.class_count[i] = np.sum(y == c)\n",
        "            self.class_prob[i] = self.class_count[i] / n_sample # Tính xác suất tiên nghiệm của mỗi lớp P(y)\n",
        "\n",
        "        self.feature_prob = np.zeros((n_class, self.n_feature)) # Lưu trữ xác suất đặc trưng xuất hiện trong mỗi lớp P(xi|y)\n",
        "\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_c = X[y == c] # Lấy tất cả các mẫu thuộc lớp c\n",
        "            n_samples_c = X_c.shape[0]\n",
        "            feature_count = np.sum(X_c > 0, axis=0) # Đếm số mẫu trong lớp c mà đặc trưng xuất hiện, khi xi = 1\n",
        "\n",
        "            self.feature_prob[i] = (feature_count + self.alpha) / (n_samples_c + 2 * self.alpha) # Laplace smoothing\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes[np.argmax(self._joint_log_likelihood(X), axis=1)]\n",
        "\n",
        "    def log_likelihood(self, X):\n",
        "        n_sample = X.shape[0]\n",
        "        n_class = len(self.classes)\n",
        "        log_likelihood = np.zeros((n_sample, n_class))\n",
        "        X_bool = X > 0 # Biến đổi X thành dạng nhị phân\n",
        "\n",
        "        for i in range(n_class):\n",
        "            log_likelihood[:, i] = np.log(self.class_prob[i])\n",
        "            feature_prob_i = self.feature_prob[i] # Tính log likelihood\n",
        "            present_features = np.log(feature_prob_i) # Tính xác suất cho đặc trưng xuất hiện (xi = 1)\n",
        "            absent_features = np.log(1 - feature_prob_i) # Tính xác suất cho đặc trưng không xuất hiện (xi = 0)\n",
        "            # Tính tổng log likelihood cho các đặc trưng xuất hiện và không xuất hiện\n",
        "            # Áp dụng công thức: Σ[x_i=1] log(P(x_i=1|y)) + Σ[x_i=0] log(P(x_i=0|y))\n",
        "            log_likelihood[:, i] += np.sum(X_bool * present_features + (~X_bool) * absent_features, axis=1)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return accuracy_score(y, y_pred)"
      ],
      "metadata": {
        "id": "0F4i-QmhT1cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Logistic Regression"
      ],
      "metadata": {
        "id": "qbShbmzZT-Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticReg:\n",
        "  def __init__(self, learning_rate=1e3, max_iter=1000):\n",
        "    self.weights = None\n",
        "    self.learning_rate = learning_rate\n",
        "    self.max_iter = max_iter\n",
        "    self.bias = 0\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -100, 100))) # Tránh tràn số\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_sample, n_feature = X.shape\n",
        "    self.weights = np.zeros(n_feature)\n",
        "\n",
        "    for i in range(self.max_iter):\n",
        "      linear_model = np.dot(X, self.weights) + self.bias\n",
        "      y_pred = self.sigmoid(linear_model)\n",
        "      dw = (1 / n_sample) * np.dot(X.T, (y_pred - y)) # Tính gradient của trọng số W\n",
        "      db = (1 / n_sample) * np.sum(y_pred - y) # Tính gradient của bias\n",
        "      self.weights -= self.learning_rate * dw # Cập nhật trọng số\n",
        "      self.bias -= self.learning_rate * db # Cập nhật bias\n",
        "\n",
        "  def predict(self, X):\n",
        "    return [1 if i > 0.5 else 0 for i in self.sigmoid(np.dot(X, self.weights) + self.bias)]\n",
        "\n",
        "  def score(self, X, y):\n",
        "    y_pred = self.predict(X)\n",
        "    return accuracy_score(y, y_pred)\n"
      ],
      "metadata": {
        "id": "JL2dkX-zT9mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Cài đặt hàm run"
      ],
      "metadata": {
        "id": "Db1b7QnHxS-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process(TrainClass0, TrainClass1, Test):\n",
        "  Trainclass0_pre = [preprocess_text(text) for text in TrainClass0] # Tiền xử lí cho Class 0\n",
        "  Trainclass1_pre = [preprocess_text(text) for text in TrainClass1] # Tiền xử lí cho Class 1\n",
        "  Test_pre = [preprocess_text(text) for text in Test] # Tiền xử lí cho tập Test\n",
        "\n",
        "  vectorize = TF_IDF()\n",
        "  X_train_pre = Trainclass0_pre + Trainclass1_pre # Tạo tập X_train\n",
        "  y_train = [0] * len(Trainclass0_pre) + [1] * len(Trainclass1_pre) # Tạo y_train\n",
        "\n",
        "  X_train = vectorize.fit_transform(X_train_pre)\n",
        "  X_test = vectorize.transform(Test_pre)\n",
        "\n",
        "  return X_train, X_test, y_train"
      ],
      "metadata": {
        "id": "obTXPsJV0YUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(TrainClass0, TrainClass1, Test):\n",
        "  X_train, X_test, y_train = process(TrainClass0, TrainClass1, Test)\n",
        "  #model = MultinomialNaiveBayes()\n",
        "  model = BernoulliNaiveBayes()\n",
        "  #model = LogisticReg()\n",
        "  model.fit(X_train, y_train)\n",
        "  return model.predict(X_test)"
      ],
      "metadata": {
        "id": "4fG6JCMlxYOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "data[\"Class\"] = label_encoder.fit_transform(data[\"Class\"])\n",
        "X = data[\"Content\"]\n",
        "y = data[\"Class\"]\n",
        "X_train, Test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "wzj6_SRs4Oxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainClass0 = X_train[y==0]\n",
        "TrainClass1 = X_train[y==1]\n"
      ],
      "metadata": {
        "id": "l369F2Xo4Rzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(run(TrainClass0, TrainClass1, Test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1qJ9uNCBk9z",
        "outputId": "2e07af40-9c78-466d-bdb6-6d8c162786f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSBhpD6cMe31"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}