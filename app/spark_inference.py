"""
Spark Inference Module (Distributed Prediction)
Demonstrates distributed inference using PySpark Pandas UDF and PhoBERT.
"""
import os
import torch
from pyspark.sql import SparkSession
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, StringType, DoubleType
import pandas as pd
from typing import Iterator

# Define schema for output (Sentiment scores for 9 aspects)
ASPECTS = [
    'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m',       # Quality, durability, materials
    'Hi·ªáu nƒÉng & Tr·∫£i nghi·ªám',   # Performance, user experience  
    'ƒê√∫ng m√¥ t·∫£',                # Accuracy of description
    'Gi√° c·∫£ & Khuy·∫øn m√£i',       # Price, discounts, value
    'V·∫≠n chuy·ªÉn',                # Shipping speed, delivery
    'ƒê√≥ng g√≥i',                  # Packaging quality
    'D·ªãch v·ª• & Th√°i ƒë·ªô Shop',    # Customer service, seller attitude
    'B·∫£o h√†nh & ƒê·ªïi tr·∫£',        # Warranty, returns
    'T√≠nh x√°c th·ª±c',             # Authenticity (fake/genuine)
]


def get_spark_session(app_name="PhoBERT Inference"):
    return SparkSession.builder \
        .appName(app_name) \
        .master("spark://spark-master:7077") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .getOrCreate()

# Load model globally on worker (Broadcast variable optimization/Singleton pattern)
model = None
tokenizer = None
device = None

def load_model_on_worker():
    """Singleton model loading on worker node."""
    global model, tokenizer, device
    if model is None:
        import sys
        sys.path.insert(0, '/app')  # Ensure app modules are visible
        from transformers import AutoTokenizer
        from phobert_trainer import PhoBERTForABSA
        
        device = torch.device('cpu')  # Use CPU on Spark workers (unless GPU config)
        
        # Load tokenizer
        tokenizer_path = "/app/models/phobert_absa/tokenizer"
        if os.path.exists(tokenizer_path):
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        else:
            tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
            
        # Load Model with new multi-task architecture
        model_path = "/app/models/phobert_absa/phobert_absa.pt"
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=device)
            state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
            
            model = PhoBERTForABSA(num_aspects=12)  # New architecture without num_labels
            model.load_state_dict(state_dict, strict=False)
            model.to(device)
            model.eval()
        else:
            raise FileNotFoundError(f"Model not found at {model_path}")

@pandas_udf("string")
def predict_batch_udf(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:
    """
    Distributed Inference UDF with multi-task model.
    Input: Iterator of text batches
    Output: Iterator of prediction results (JSON string)
    """
    # Initialize model once per executor/python worker
    load_model_on_worker()
    
    import json
    
    # Label map for display
    label_map = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}
    
    for texts in batch_iter:
        # Preprocessing batch
        inputs = tokenizer(
            texts.tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        ).to(device)
        
        # Inference with multi-task model
        with torch.no_grad():
            logits_m, logits_s = model(inputs['input_ids'], inputs['attention_mask'])
            
            # Mention predictions (binary)
            preds_m = (torch.sigmoid(logits_m) > 0.5).cpu().numpy()
            
            # Sentiment predictions (0=NEG, 1=POS, 2=NEU)
            preds_s = torch.argmax(logits_s, dim=-1).cpu().numpy()
            
        # Format results
        results = []
        
        for i in range(len(texts)):
            row_res = {}
            for j, aspect in enumerate(ASPECTS):
                if preds_m[i][j]:  # Aspect is mentioned
                    row_res[aspect] = label_map[preds_s[i][j]]
            results.append(json.dumps(row_res, ensure_ascii=False))
            
        yield pd.Series(results)

if __name__ == "__main__":
    spark = get_spark_session()
    
    # Mock Big Data (1000 rows)
    raw_data = [
        ("S·∫£n ph·∫©m x√†i ·ªïn, giao h√†ng h∆°i l√¢u",),
        ("Tuy·ªát v·ªùi √¥ng m·∫∑t tr·ªùi",),
        ("M√°y n√≥ng qu√°, pin t·ª•t nhanh",),
        ("Shop ƒë√≥ng g√≥i c·∫©n th·∫≠n, cho 5 sao",)
    ] * 250
    
    df = spark.createDataFrame(raw_data, ["review_text"])
    df = df.repartition(4)  # Simulate multiple partitions
    
    print("üöÄ Starting Distributed Inference for 1000 reviews...")
    
    # Apply Inference UDF
    df_pred = df.withColumn("prediction", predict_batch_udf("review_text"))
    
    # Show results
    df_pred.select("review_text", "prediction").show(10, truncate=False)
    
    spark.stop()
    print("‚úÖ Inference complete.")
