{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2M8R-Fzx2xs"
      },
      "source": [
        "Nh√≥m 18\n",
        "- ƒê·ªó T·∫•n L·ª±c - 23520903\n",
        "- L√™ Quang Long - -23520878"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYn6SwFLXl-T"
      },
      "source": [
        "#### Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho x·ª≠ l√Ω d·ªØ li·ªáu, x√¢y d·ª±ng m√¥ h√¨nh, v√† Spark\n",
        "\n",
        "- pandas, numpy: X·ª≠ l√Ω d·ªØ li·ªáu.\n",
        "- torch, torch.nn, torch.optim: X√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh deep learning.\n",
        "- sklearn: Ti·ªÅn x·ª≠ l√Ω, chia t·∫≠p, ƒë√°nh gi√° m√¥ h√¨nh.\n",
        "- re: X·ª≠ l√Ω chu·ªói, regex.\n",
        "- kagglehub: T·∫£i dataset t·ª´ Kaggle.\n",
        "- pyspark: X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn v·ªõi Spark, ƒë·ªãnh nghƒ©a UDF.\n",
        "- transformers: S·ª≠ d·ª•ng m√¥ h√¨nh BERT (PhoBERT)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:24:43.995028Z",
          "iopub.status.busy": "2025-10-07T07:24:43.994545Z",
          "iopub.status.idle": "2025-10-07T07:24:44.000485Z",
          "shell.execute_reply": "2025-10-07T07:24:43.999592Z",
          "shell.execute_reply.started": "2025-10-07T07:24:43.995002Z"
        },
        "id": "b3agSukMo_oz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "import re\n",
        "import kagglehub\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType, ArrayType, StringType, IntegerType\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.ml.functions import predict_batch_udf\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqFqiXJOXl-T"
      },
      "source": [
        "#### T·∫£i dataset ABSA ti·∫øng Vi·ªát t·ª´ Kaggle\n",
        "\n",
        "- S·ª≠ d·ª•ng kagglehub ƒë·ªÉ t·∫£i v·ªÅ b·ªô d·ªØ li·ªáu.\n",
        "- ƒê·ªçc c√°c file train, test, val b·∫±ng pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:24:44.295866Z",
          "iopub.status.busy": "2025-10-07T07:24:44.295587Z",
          "iopub.status.idle": "2025-10-07T07:24:44.473299Z",
          "shell.execute_reply": "2025-10-07T07:24:44.472431Z",
          "shell.execute_reply.started": "2025-10-07T07:24:44.295818Z"
        },
        "id": "sUOxjTB5yC75",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "path = kagglehub.dataset_download(\"cthng123/absa-vietnamese\")\n",
        "\n",
        "train = pd.read_csv(f\"{path}/train_data.csv\")\n",
        "test = pd.read_csv(f\"{path}/test_data.csv\")\n",
        "val = pd.read_csv(f\"{path}/val_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTSX3G6RP3Zv"
      },
      "source": [
        "## BERT + Predict batch UDF\n",
        "\n",
        "- S·ª≠ d·ª•ng m√¥ h√¨nh BERT (PhoBERT) ƒë·ªÉ d·ª± ƒëo√°n song song tr√™n Spark DataFrame.\n",
        "- Tr·∫£ v·ªÅ d·ª± ƒëo√°n cho t·ª´ng review d∆∞·ªõi d·∫°ng chu·ªói.\n",
        "- ƒê√°nh gi√° l·∫°i ƒë·ªô ch√≠nh x√°c tr√™n Spark DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b01b896"
      },
      "source": [
        "#### C√†i ƒë·∫∑t th∆∞ vi·ªán transformers\n",
        "\n",
        "- S·ª≠ d·ª•ng pip ƒë·ªÉ c√†i ƒë·∫∑t phi√™n b·∫£n `transformers==4.40.2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:32:28.968401Z",
          "iopub.status.busy": "2025-10-07T07:32:28.967797Z",
          "iopub.status.idle": "2025-10-07T07:32:32.286180Z",
          "shell.execute_reply": "2025-10-07T07:32:32.285126Z",
          "shell.execute_reply.started": "2025-10-07T07:32:28.968374Z"
        },
        "id": "lulGDC4CPiiL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.40.2 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### T·∫£i tokenizer v√† m√¥ h√¨nh PhoBERT\n",
        "\n",
        "- T·∫£i `AutoTokenizer` v√† `AutoModel` t·ª´ th∆∞ vi·ªán `transformers`.\n",
        "- Kh·ªüi t·∫°o tokenizer cho m√¥ h√¨nh `vinai/phobert-base`."
      ],
      "metadata": {
        "id": "lQ44o49YYswY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:32:32.287975Z",
          "iopub.status.busy": "2025-10-07T07:32:32.287700Z",
          "iopub.status.idle": "2025-10-07T07:32:32.823247Z",
          "shell.execute_reply": "2025-10-07T07:32:32.822384Z",
          "shell.execute_reply.started": "2025-10-07T07:32:32.287953Z"
        },
        "trusted": true,
        "id": "YNyURrKEXl-X",
        "outputId": "b1d63a23-5bb3-4f46-c7fe-2bb358fdff6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ƒê·ªãnh nghƒ©a m√¥ h√¨nh ABSA d·ª±a tr√™n BERT v√† kh·ªüi t·∫°o\n",
        "\n",
        "- ƒê·ªãnh nghƒ©a l·ªõp `BERT_ABSA` k·∫ø th·ª´a t·ª´ `nn.Module`.\n",
        "- M√¥ h√¨nh bao g·ªìm l·ªõp BERT (`vinai/phobert-base`), dropout, v√† l·ªõp tuy·∫øn t√≠nh ƒë·ªÉ d·ª± ƒëo√°n ph√¢n lo·∫°i cho t·ª´ng kh√≠a c·∫°nh.\n",
        "- Kh·ªüi t·∫°o m√¥ h√¨nh, h√†m m·∫•t m√°t `CrossEntropyLoss`, v√† optimizer `Adam`.\n",
        "- Chuy·ªÉn m√¥ h√¨nh sang thi·∫øt b·ªã (CPU/GPU) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·ªüi bi·∫øn `DEVICE`."
      ],
      "metadata": {
        "id": "5kfjZh6xYvKA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:32:35.252345Z",
          "iopub.status.busy": "2025-10-07T07:32:35.252082Z",
          "iopub.status.idle": "2025-10-07T07:32:36.105246Z",
          "shell.execute_reply": "2025-10-07T07:32:36.104583Z",
          "shell.execute_reply.started": "2025-10-07T07:32:35.252327Z"
        },
        "id": "Ughhk6CdZ09t",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class BERT_ABSA(nn.Module):\n",
        "    def __init__(self, num_aspects=len(ASPECTS), num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(768, num_aspects * num_classes)\n",
        "        self.num_classes = num_classes\n",
        "        self.num_aspects = num_aspects\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.pooler_output\n",
        "        drop = self.dropout(pooled_output)\n",
        "        out = self.fc(drop)\n",
        "        return out.view(-1, self.num_aspects, self.num_classes)\n",
        "\n",
        "\n",
        "model_bert = BERT_ABSA().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_bert.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Token h√≥a d·ªØ li·ªáu v√† chu·∫©n b·ªã Tensor\n",
        "\n",
        "- ƒê·ªãnh nghƒ©a h√†m `tokenize` ƒë·ªÉ chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh input_ids v√† attention_mask s·ª≠ d·ª•ng tokenizer c·ªßa PhoBERT.\n",
        "- √Åp d·ª•ng h√†m `tokenize` cho c·ªôt 'Review' trong c√°c t·∫≠p train, test, v√† val.\n",
        "- N·ªëi c√°c tensor input_ids v√† attention_mask l·∫°i v·ªõi nhau cho t·ª´ng t·∫≠p d·ªØ li·ªáu."
      ],
      "metadata": {
        "id": "P36XVQqVYxbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:32:37.812499Z",
          "iopub.status.busy": "2025-10-07T07:32:37.811732Z",
          "iopub.status.idle": "2025-10-07T07:32:40.568379Z",
          "shell.execute_reply": "2025-10-07T07:32:40.567339Z",
          "shell.execute_reply.started": "2025-10-07T07:32:37.812473Z"
        },
        "id": "2z6XPTmOPiCi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return tokenizer.encode_plus(text, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "train_bert = [tokenize(text) for text in train['Review']]\n",
        "test_bert = [tokenize(text) for text in test['Review']]\n",
        "val_bert = [tokenize(text) for text in val['Review']]\n",
        "\n",
        "train_input_ids = torch.cat([e['input_ids'] for e in train_bert])\n",
        "train_attention_mask = torch.cat([e['attention_mask'] for e in train_bert])\n",
        "val_input_ids = torch.cat([e['input_ids'] for e in val_bert])\n",
        "val_attention_mask = torch.cat([e['attention_mask'] for e in val_bert])\n",
        "test_input_ids = torch.cat([e['input_ids'] for e in test_bert])\n",
        "test_attention_mask = torch.cat([e['attention_mask'] for e in test_bert])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ƒê·ªãnh nghƒ©a Dataset, DataLoader\n",
        "\n",
        "- ƒê·ªãnh nghƒ©a l·ªõp `ABSADataset_Bert` k·∫ø th·ª´a t·ª´ `Dataset` ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu cho PyTorch.\n",
        "- T·∫°o instance c·ªßa Dataset cho t·∫≠p train, test, v√† val.\n",
        "- T·∫°o DataLoader cho t·ª´ng t·∫≠p d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω theo batch.\n"
      ],
      "metadata": {
        "id": "usROUFrKYy74"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:32:41.867769Z",
          "iopub.status.busy": "2025-10-07T07:32:41.867463Z",
          "iopub.status.idle": "2025-10-07T07:36:03.211705Z",
          "shell.execute_reply": "2025-10-07T07:36:03.210678Z",
          "shell.execute_reply.started": "2025-10-07T07:32:41.867746Z"
        },
        "id": "wtLOF4S7W6VT",
        "trusted": true,
        "outputId": "d8d2369a-39fb-4e05-c47d-169d27adb41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Loss: 5.652547785730073\n",
            "Epoch 2, Val Loss: 0.0, Val Accuracy: 0.7489316239316239\n"
          ]
        }
      ],
      "source": [
        "class ABSADataset_Bert(Dataset):\n",
        "      def __init__(self, input_ids, attention_masks, labels):\n",
        "            self.input_ids = input_ids\n",
        "            self.attention_masks = attention_masks\n",
        "            self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "            return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]\n",
        "\n",
        "      def __len__(self):\n",
        "            return len(self.input_ids)\n",
        "\n",
        "\n",
        "train_data = ABSADataset_Bert(train_input_ids, train_attention_mask, train_labels)\n",
        "test_data = ABSADataset_Bert(test_input_ids, test_attention_mask, test_labels)\n",
        "val_data = ABSADataset_Bert(val_input_ids, val_attention_mask, val_labels)\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_load = DataLoader(test_data, batch_size=32)\n",
        "val_load = DataLoader(val_data, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh BERT\n",
        "\n",
        "- Th·ª±c hi·ªán v√≤ng l·∫∑p hu·∫•n luy·ªán m√¥ h√¨nh:\n",
        "    - ƒê·∫∑t m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô train (`model_bert.train()`).\n",
        "    - T√≠nh t·ªïng m·∫•t m√°t qua t·ª´ng batch.\n",
        "    - Th·ª±c hi·ªán backpropagation v√† c·∫≠p nh·∫≠t tr·ªçng s·ªë.\n",
        "    - In ra m·∫•t m√°t sau m·ªói epoch.\n",
        "- ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation sau khi hu·∫•n luy·ªán:\n",
        "    - ƒê·∫∑t m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô eval (`model_bert.eval()`).\n",
        "    - D·ª± ƒëo√°n tr√™n t·∫≠p validation.\n",
        "    - T√≠nh to√°n ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p validation.\n",
        "    - In ra ƒë·ªô ch√≠nh x√°c.\n",
        "- L∆∞u tr·∫°ng th√°i c·ªßa m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán v√†o file 'bert_absa_model.pth'."
      ],
      "metadata": {
        "id": "8MYBqcO2Y5fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 1\n",
        "for e in range(epoch):\n",
        "    model_bert.train()\n",
        "    total_loss = 0\n",
        "    for input_ids, attention_mask, labels in train_load:\n",
        "        input_ids, attention_mask, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model_bert(input_ids, attention_mask)\n",
        "        loss = sum(criterion(output[:, i, :], labels[:, i]) for i in range(len(ASPECTS)))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {e+1}/{epoch}, Loss: {total_loss/len(train_load)}\")\n",
        "\n",
        "# EVAL\n",
        "model_bert.eval()\n",
        "val_pred = []\n",
        "val_true = []\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in val_load:\n",
        "        input_ids, attention_mask, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model_bert(input_ids, attention_mask)\n",
        "        val_pred.extend(torch.argmax(outputs, dim=2).cpu().numpy())\n",
        "        val_true.extend(labels.cpu().numpy())\n",
        "val_loss /= len(val_load)\n",
        "val_acc = accuracy_score(np.array(val_true).flatten(), np.array(val_pred).flatten())\n",
        "print(f'Epoch {epoch+1}, Val Loss: {val_loss}, Val Accuracy: {val_acc}')\n",
        "\n",
        "torch.save(model_bert.state_dict(), 'bert_absa_model.pth')"
      ],
      "metadata": {
        "id": "A6EXXx6nY3RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test\n",
        "\n",
        "- ƒê·∫∑t m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô eval (`model_bert.eval()`).\n",
        "- Th·ª±c hi·ªán d·ª± ƒëo√°n tr√™n t·∫≠p test.\n",
        "- So s√°nh k·∫øt qu·∫£ d·ª± ƒëo√°n v·ªõi nh√£n th·ª±c t·∫ø.\n",
        "- T√≠nh to√°n ƒë·ªô ch√≠nh x√°c cu·ªëi c√πng tr√™n t·∫≠p test v√† in ra."
      ],
      "metadata": {
        "id": "VpcDWJXVY7bw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T07:36:55.330923Z",
          "iopub.status.busy": "2025-10-07T07:36:55.330133Z",
          "iopub.status.idle": "2025-10-07T07:37:12.151766Z",
          "shell.execute_reply": "2025-10-07T07:37:12.151017Z",
          "shell.execute_reply.started": "2025-10-07T07:36:55.330894Z"
        },
        "id": "adqEgUSOtEfw",
        "trusted": true,
        "outputId": "21884beb-e34a-4d38-dbc5-3af932323d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 0.753792735042735\n"
          ]
        }
      ],
      "source": [
        "model_bert.eval()\n",
        "test_preds = []\n",
        "test_true = []\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in test_load:\n",
        "        input_ids, attention_mask, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model_bert(input_ids, attention_mask)\n",
        "        test_preds.extend(torch.argmax(outputs, dim=2).cpu().numpy())\n",
        "        test_true.extend(labels.cpu().numpy())\n",
        "test_acc = accuracy_score(np.array(test_true).flatten(), np.array(test_preds).flatten())\n",
        "print('Final Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAMiC0K3Xl-b"
      },
      "source": [
        "#### D·ª± ƒëo√°n song song v·ªõi Spark predict_batch_udf (BERT)\n",
        "\n",
        "- S·ª≠ d·ª•ng Spark ƒë·ªÉ d·ª± ƒëo√°n song song theo batch tr√™n t·∫≠p test v·ªõi m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán.\n",
        "- Tr·∫£ v·ªÅ d·ª± ƒëo√°n cho t·ª´ng review d∆∞·ªõi d·∫°ng m·∫£ng s·ªë.\n",
        "- ƒê√°nh gi√° l·∫°i ƒë·ªô ch√≠nh x√°c tr√™n Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T08:02:31.992549Z",
          "iopub.status.busy": "2025-10-07T08:02:31.992035Z",
          "iopub.status.idle": "2025-10-07T08:02:44.041025Z",
          "shell.execute_reply": "2025-10-07T08:02:44.039849Z",
          "shell.execute_reply.started": "2025-10-07T08:02:31.992525Z"
        },
        "trusted": true,
        "id": "VD2d_DNVXl-b",
        "outputId": "d7357506-8565-438f-fde6-5d29aa922232"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
            "|Review                                                                                                                                                                                                                                               |predictions             |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
            "|Gi√†y h∆°i c√≥ m√πi n·ªìng, l∆∞u √Ω ƒë√¥i LA kh√¥ng ph·∫£i ƒë·∫ø x√°m n√™n mng c√¢n nh·∫Øc kƒ© nh√© ·∫° nhma v·ªõi gi√° ti·ªÅn n√†y th√¨ l√† oke                                                                                                                                      |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|H√†ng v·ªÅ ƒë·∫πp l·∫Øm nha ship th√¢n thi·ªán ƒëi gi√†y v·ª´a in‚ù§Ô∏è                                                                                                                                                                                                 |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|H√†ng √¥k n√™n mua D√†y r·∫•t ƒë·∫πp                                                                                                                                                                                                                          |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|Bun. GTI g·ª≠i Oke  s·ªõ ∆° ƒëi sidbd. B·ªüi ƒëi ƒë∆∞·ª£c ƒë·∫°n s√≠bd ke √† ƒëi d√≠nh s∆∞·ª£ng.                                                                                                                                                                            |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|M√†u ƒë·∫πp gi·ªëng trong h√¨nh m·ªçi ng∆∞·ªùi n√™n mua nha M√¨nh th·∫•y c·ª±c k·ª≥ ƒë·∫πp lu√¥n √Ω                                                                                                                                                                           |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|ch·∫•t l∆∞·ª£ng ph√π h·ª£p v·ªõi gi√° ti·ªÅn ƒëi ƒë√∫ng sz nh∆∞ng  h∆°i r·ªông x√≠u ·∫°                                                                                                                                                                                     |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|Gi√†y tr∆∞·ª£t l·∫Øm huhu ƒê√°nh gi·∫£i tr∆∞·ªùng m√† tr∆∞·ª£t tr√™n s√¢n nh∆∞ tr∆∞·ª£t patin lu√¥nüíî ƒê∆∞·ª£c c√°i ngo·∫°i h√¨nh ƒë·∫πp Mua v·ªõi gi√° 200 √Ω nhma l∆∞·ªùi ch·ªçn kh√¥ng bi·∫øt ch·ªçn c√°i mua ƒë·∫°i                                                                                   |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|Tr ∆°i d√©p ƒë·∫πp vs dth l·∫Øm nha vs gi√° n√†y m√† ch·∫•t l∆∞·ª£ng v qu√° to·∫πt v·ªùi m√¨nh th·∫•y mn n√™n ƒëi ƒë√∫ng size                                                                                                                                                   |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|C≈©ng t·∫°m ƒë∆∞·ª£c thoi                                                                                                                                                                                                                                   |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|Shop h·ªó tr·ª£ r·∫•t t·ªët. Mn n√™n mua nh√©                                                                                                                                                                                                                  |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|Sp t·ªët nha                                                                                                                                                                                                                                           |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|Ch∆∞a th·ª≠ nh∆∞ng th·∫•y kh√° ·ªïn.                                                                                                                                                                                                                          |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|giao h√†ng nhanh, s·∫£n ph·∫©m t·ªët, m√¨nh r·∫•t h√†i l√≤ng, t·∫∑ng shop 5 sao, c√°m ∆°n shop r·∫•t nhi·ªÅu                                                                                                                                                             |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|ƒêi √™m, gi√†y ƒë·∫πp . Giao h√†ng nhanh . Cho shop 10 ƒëierm                                                                                                                                                                                                |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|ƒê√£ nh·∫≠n ƒë∆∞·ª£c h√†ng nhanh ch√≥ng ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m t·ªët ncl okk                                                                                                                                                                                        |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|S·∫£n ph·∫©m t·ªët ƒë·∫πp 10‚Ç´                                                                                                                                                                                                                                 |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|H√†ng ƒë√≥ng g√≥i r·∫•t ƒë·∫πp. Qu·∫£ nhi√™n ng∆∞·ªùi b√°n ƒë√£ g·ª≠i g·∫Øm r·∫•t nhi·ªÅu t√¢m t∆∞ v√† t√¨nh c·∫£m trong l√∫c g√≥i h√†ng. H·ªôp kh√¥ng c√≥ d·∫•u hi·ªáu b·ªã m√≥p, ch·ª©ng t·ªè b·ªô ph·∫≠n v·∫≠n chuy·ªÉn ƒë√£ n√¢ng nh∆∞ n√¢ng tr·ª©ng h·ª©ng nh∆∞ h·ª©ng hoa. xin nghi√™ng m√¨nh k√≠nh c·∫©n c·∫£m ∆°n v√¨ t·∫•t c·∫£|[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|S·∫£n ph·∫©m t·ªët giao h√†ng nhanh c√≥ ƒëk s·∫Ω ·ªßng h·ªô shop ti·∫øp nh√© ok vvv                                                                                                                                                                                    |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|Gi√†y ƒë·∫πpppp nha mn N√™nnnn muaaa ·∫° m√¨nh mua sz 37 v·ª´a ch√¢n lu√¥nn                                                                                                                                                                                      |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "|ƒê·∫ø nh·ª±a n√™n r·∫•t tr∆°n.ƒëi n·ªÅn b√≥ng tr∆∞·ª£t nh∆∞ b√¥i d·∫ßu. ƒêi ƒë·ª©ng l√¢u s·∫Ω ƒëau ch√¢n                                                                                                                                                                          |[0, 0, 2, 0, 0, 0, 0, 0]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Spark Test Accuracy: 0.753792735042735\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName('ABSA_PredictBatchUDFBERT').getOrCreate()\n",
        "spark_test_df = spark.createDataFrame(test[['Review']])\n",
        "\n",
        "def predict_batch_bert():\n",
        "\n",
        "    def predict_batch(reviews):\n",
        "        load_model = BERT_ABSA().to('cpu')\n",
        "        load_model.load_state_dict(torch.load('bert_absa_model.pth'))\n",
        "        load_model.eval()\n",
        "\n",
        "        tokens = [tokenize(t) for t in reviews]\n",
        "        input_ids = torch.cat([t['input_ids'] for t in tokens])\n",
        "        attention_mask = torch.cat([t['attention_mask'] for t in tokens])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = load_model(input_ids, attention_mask)\n",
        "            predict = torch.argmax(output, dim=2).numpy()\n",
        "        return predict\n",
        "\n",
        "    return predict_batch\n",
        "\n",
        "absa_batch_udf_bert = predict_batch_udf(\n",
        "    predict_batch_bert,\n",
        "    return_type=ArrayType(IntegerType()),\n",
        "    batch_size=32,\n",
        "    ## input_tensor_shapes=[[1]]\n",
        ")\n",
        "\n",
        "# Apply UDF\n",
        "result_df_udf = spark_test_df.withColumn('predictions', absa_batch_udf_bert(spark_test_df['Review']))\n",
        "result_df_udf.show(truncate=False)\n",
        "\n",
        "preds_df = spark.createDataFrame(pd.DataFrame({'prediction': np.array(test_preds).flatten(), 'label': np.array(test_true).flatten()}))\n",
        "preds_df = preds_df.withColumn('prediction', preds_df['prediction'].cast(DoubleType()))\n",
        "preds_df = preds_df.withColumn('label', preds_df['label'].cast(DoubleType()))\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(preds_df)\n",
        "print('Spark Test Accuracy:', accuracy)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "YH19-VoiXl-b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5006900,
          "isSourceIdPinned": false,
          "sourceId": 8412310,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31153,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}